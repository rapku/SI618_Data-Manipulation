{"cells":[{"cell_type":"markdown","source":["# SI 618 Homework 8 - Improving LDA\n\n## Objectives\n* to gain practical experience with NLP techniques\n* to be exposed to loading large datasets via AWS S3 and parquet format\n\n## Please fill in...\n### * Your name: Raphael Ku\n### * People you worked with:  I worked by myself\n\n## Submission Instructions:\nPlease turn in your completed Databricks notebook in HTML format as well as the URL to the published version of your completed notebook.\n\n## Assignment Instructions:\nIn this week's lab, we investigated the use of latent Dirichilet allocation (LDA) to analyze text.\nIn particular, we applied LDA to the Enron Corp. email data.  In this homework assignment we\nare going to ask you to revisit the Enron LDA analysis and try to improve it.  \n\nRecall that LDA seeks to extract a number of topics (the number is supplied by you) from a \ncollection of documents. Each one of those topics can be described by the words that are most closely\nassociated with it and thereby facilitate the interpretation of the topic.  \nFor example, a topic that is most closely associated with the words blue, green, \nyellow, red, and purple might be interpreted as being about \"colors\".  That's an ideal\nexample.  In practice, the words that are associated with the topics often don't lead us to \nan easy interpretation of the topic.  In some cases, we can improve the interpretability of\nthe topics.  \n\nFor example, we can manipulate the model parameters (e.g. changing the number of topics) or\nwe can try to do a better job of cleaning the data before analyzing it.  We can experiment\nwith the inclusion or exclusion of stopwords.  Or we can get very creative and use bigrams or\ntrigrams instead of unigrams (words) in our analysis.\n\nThis homework assignment provides you with an oppportunity to improve the LDA we performed on\nthe Enron data, which is reproduced below.  To start this lab, run the cells below and examine the\noutput. Describe the topics and comment on the quality and/or interpretability of the topics. Then, follow the steps below to apply some of the techniques mentioned above.\n \nOne measure of the \"goodness\" of a topic model is the interpretability of the topics.  That is,\ndo the words associated with the topic form a coherent set (like the colors example above) or\nare the seemingly random words?\n\nYou will notice that we're using a 1% sample of the email corpus (note the ```sample(0.01)``` function). Another measure of the \"goodness\" of a topic model is the stability of the model over different random samples.  \nWhat happens to your topics when you re-run the analysis (thereby sampling a differnt 1%).  What happens when you\nrun your analysis on the complete email corpus?\n\nThere are also two numerical measures of model goodness that are available:  log(perplexity) and log(likelihood).\nLower values of log(perplexity) are better, whereas higher values of log(likelihood) are generally\nconsidered better.   You can use these \"objective\" measures in combination with the \"subjective\" assessments of \nthe interpretability of topics when assessing your model.\n\nThis assignment is worth a total of 80 points.  You will receive up to 16 points for each of the following 4 improvements:\n1. Vary the number of topics from 6 to 12 (i.e. 6, 7, 8, 9, 10, 11, and 12).  Which value(s) gives you the \"best\" solution?  What criteria did you use for determining how good each solution is? \n2. How does the topic model change if you include or exclude stopwords? What's the best way to deal with non-alpha characters (e.g. numbers)? Is it better to include or exclude stopwords?  Use the \"better\" version in subsequent steps.\n3. Clean the text from the body of each email message by excluding the \"quoted replies\" (i.e. the copy of the original message\nthat is often included in a reply).  How do the results of your topic model change? (Note: you might want to use RDDs and regular expressions for part of this analysis.)\n4. Given the model from the \"best\" number of topics from Step 1, the best choice of including or excluding stopwords, \nand using cleaned email bodies, how consistent/stable are the topics from \nmultiple runs (i.e. using different 1% samples)?  How do you define consistency and stability?\n\nFor each improvement, you will be assessed on:\n\n1. the clarity of your code (both in terms of programming aspects such as variable names and in terms of Markdown cells explaining what you did),\n2. the completeness of your interpretations, and\n3. the quality of the presentation of your results (e.g. using tables and/or visualizations as appropriate).\n\n### Above and Beyond\nSelect one of the following options for up to 16 points:\n1. Use LDA to create two additional topic models based on (1) bigrams and (2) trigrams.  Find the best number of topics, determine whether to include\nstopwords, and clean the email bodies.  How do these topics compare with the ones from the unigram analysis above in terms of interpretability and stability?\n2. In the cell below, the LDA model of the enron DataFrame is stored in a DataFrame called ```enron_lda```.  If you examine that DataFrame you will notice a column called ```topicDistribution```, which tells you the proportion of each topic that makes up each document.  For each document (i.e. row in ```enron_lda```), figure out which topic is the dominant one and label that document as belonging to that topic.  So, for example, if you \nhave a 6-topic model and see\n```\ntopicDistribution=DenseVector([0.011, 0.0114, 0.9446, 0.0111, 0.011, 0.0109]\n```\nfor a document, you would label that document as topic \"3\" because the largest number (0.9446) is associated with topic #3.  Based on this approach, report the number of documents that are labelled with each topic number.\n\n### End of instructions... code follows"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer \nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.pipeline import Pipeline\nimport numpy as np\nimport nltk\nnltk.download(\"book\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading collection &apos;book&apos;\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection book\n<span class=\"ansired\">Out[</span><span class=\"ansired\">218</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords as nltkstopwords"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# This allows us to use the S3 bucket as a directory containing the data file(s)\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"\"\nMOUNT_NAME = \"\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n#display(dbutils.fs.ls(\"/mnt/umsi-data-science/si618wn2017\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/umsi-data-science/ has been unmounted.\n<span class=\"ansired\">Out[</span><span class=\"ansired\">220</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# This is a helper function that looks up the words associated with indices.  \n# It's used below.\nfrom pyspark.sql.types import ArrayType, StringType\n\ndef indices_to_terms(vocabulary):\n    def indices_to_terms(xs):\n        return [vocabulary[int(x)] for x in xs]\n    return udf(indices_to_terms, ArrayType(StringType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# The next line loads the Enron email dataset from parquet format.  For details, see\n# https://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files\n# Note the following line takes a sample of approximately 1% of the rows\nenron = spark.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False,0.01)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# This cell is a complete machine learning pipeline to run LDA on a dataset\n# Note that you might want to split this up into individual cells for\n# your assignment.  \n\nk = 6 # set the number of topics to extract\n\ntokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\n\nvectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2) \n\nprint (\"k = \",k)\nlda = LDA(k=6, maxIter=10)\n\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\nprint(\"Vocab length is\",len(cmv))\n\nldaModel = pipelineModel.stages[-1]\n\n# Assess the model\nenron_lda = pipelineModel.transform(enron)\n\nlp = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity  (lower is better): \",lp)\nll = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood (higher is better): \",ll)\n# Describe topics.\n\ntopics = ldaModel.describeTopics(8)\n\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(10,truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">k =  6\nVocab length is 42848\nLog perplexity  (lower is better):  6.618557579796248\nLog likelihood (higher is better):  -8892678.562574387\n+--------------------------------------------------------------------------------------------+\ntopicWords                                                                                  |\n+--------------------------------------------------------------------------------------------+\n[, =09=09, ab, to:, subject:, -, cc:, enron]                                                |\n[class=td2, &lt;tr, &gt;&lt;td, &gt;&lt;a, size=1&gt;updated, face=&quot;arial,, helvetica&quot;&gt;&lt;font, bgcolor=#eaeaea]|\n[&gt;, , virus, pnm, western, task, football., k]                                              |\n[, &gt;, -, to:, subject:, enron, pm, please]                                                  |\n[database, attempting, occurred, engine, (error, !!!an, $2a04), initialize]                 |\n[, please, -, to:, subject:, *, let, know]                                                  |\n+--------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Question 1\nAs part of the problem, to easily modify the number of topics, I converted the LDA process into a function. The criteria I plan to use to evaluate the groupings would be on log likelihood, and interpretability of the topics.\n\n\nBased on the 1% sample chosen, the best number of topics seem to be k=9, as seen in <a href='https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3809935650681397'>Cmd 13</a>, given a log likelihood not too far from the lowest of the modeled (-8.9M for k=9 vs. the lowest log likelihood of -8.81M for k=7), as well as stronger groupings of words for database issues (topic 1), concerns on business in certain states (topic 5), and grouping words with similar structure (management-*) in topic 2."],"metadata":{}},{"cell_type":"code","source":["### LDA (function so k is easily modified)\ndef LDAfunc(kval, data):\n  tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n\n  stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  stopWordsRemover.loadDefaultStopWords(\"english\")\n\n  vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2) \n  \n  lda = LDA(k=kval, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\n  pipelineModel = pipeline.fit(data)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n  print(\"Vocab length is\",len(cmv))\n  \n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(data)\n  \n  lp = ldaModel.logPerplexity(enron_lda)\n  print(\"Log perplexity  (lower is better): \",lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  print(\"Log likelihood (higher is better): \",ll)\n  # Describe topics.\n\n  topics = ldaModel.describeTopics(8)\n\n  topics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\n  topics.select(\"topicWords\").show(20,truncate=False)\n  \n  return enron_lda"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["LDAfunc(6, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.616924319668698\nLog likelihood (higher is better):  -8890474.313496111\n+---------------------------------------------------------------------+\ntopicWords                                                           |\n+---------------------------------------------------------------------+\n[, database, error, attempting, occurred, engine, initialize, (error]|\n[tax), (enw), (eel), (ena), (isc), (corp), (corp, (enw]              |\n[&gt;, class=td2, , &gt;&lt;td, 25mw, 9/30, size=1&gt;updated, &gt;&lt;a]              |\n[, &gt;, -, to:, subject:, enron, pm, please]                           |\n[&gt;, , (e-mail);, virus, state, to:, please, desktop]                 |\n[, class=td2, variances, hourahead, final, ----, hour:, -]           |\n+---------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">225</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["LDAfunc(7, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.558330561255335\nLog likelihood (higher is better):  -8811753.526933689\n+----------------------------------------------------------------------------+\ntopicWords                                                                  |\n+----------------------------------------------------------------------------+\n[, -, power, subject:, to:, gas, day&apos;s, =]                                  |\n[(enw), tax), (eel), (ena), (isc), (corp), (corp, (enw]                     |\n[&gt;, class=td2, , &lt;tr, &gt;&lt;td, &gt;&lt;a, helvetica&quot;&gt;&lt;font, size=1&gt;updated]          |\n[, &gt;, -, pm, to:, subject:, please, =]                                      |\n[database, borland, occurred, attempting, (error, !!!an, engine, initialize]|\n[, please, -, =20, ,, following, group, &lt;&lt;&lt;&gt;&gt;&gt;]                             |\n[, enron, new, -, to:, subject:, please, =20]                               |\n+----------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">226</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["LDAfunc(8, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.610637768387907\nLog likelihood (higher is better):  -8882033.77218961\n+--------------------------------------------------------------------------------------------+\ntopicWords                                                                                  |\n+--------------------------------------------------------------------------------------------+\n[, -, subject:, to:, cc:, enron, following, pm]                                             |\n[class=td2, &gt;&lt;td, &lt;tr, helvetica&quot;&gt;&lt;font, size=1&gt;updated, &gt;&lt;a, face=&quot;arial,, bgcolor=#eaeaea]|\n[(enw), tax), (ena), (isc), (eel), (corp), (corp, (enw]                                     |\n[, &gt;, -, to:, pm, subject:, =20, cc:]                                                       |\n[, energy, state, williams, 7, please, news, live]                                          |\n[, =20, please, ab, *, committee, &lt;&lt;&lt;&gt;&gt;&gt;, may]                                              |\n[, enron, new, said, please, company, to:, -]                                               |\n[database, $2a04), borland, engine, !!!an, occurred, (error, initialize]                    |\n+--------------------------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">227</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["LDAfunc(9, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.635198078097851\nLog likelihood (higher is better):  -8915032.758847129\n+----------------------------------------------------------------------------------------------------------------+\ntopicWords                                                                                                      |\n+----------------------------------------------------------------------------------------------------------------+\n[, database, error, engine, (error, !!!an, occurred, initialize]                                                |\n[1/17/01, eff_dt, management-pwr, management-coal, management-gas, management-crd, portfolio_id, agg-management]|\n[class=td2, &gt;&lt;td, &gt;&lt;a, helvetica&quot;&gt;&lt;font, &lt;tr, size=1&gt;updated, face=&quot;arial,, bgcolor=#f0f0f0]                    |\n[, &gt;, -, pm, to:, subject:, =, please]                                                                          |\n[, state, tax), energy, power, please, contracts, (enw)]                                                        |\n[, ,, power, -, to:, said, may, gas]                                                                            |\n[, enron, -, to:, subject:, =20, please, new]                                                                   |\n[&gt;, (enw), tax), , (ena), (eel), (isc), (corp)]                                                                 |\n[&lt;&lt;&lt;&gt;&gt;&gt;, 25mw, 9/30, , helvetica,, size=&quot;3&quot;, cellspacing=&quot;0&quot;, color=&quot;#ffcc33&quot;&gt;&lt;font]                            |\n+----------------------------------------------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">228</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["LDAfunc(10, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.711788345946857\nLog likelihood (higher is better):  -9017939.86926176\n+----------------------------------------------------------------------------+\ntopicWords                                                                  |\n+----------------------------------------------------------------------------+\n[, energy, ab, power, gas, iep, shows, enron]                               |\n[, like,, back?, production, may, anything, similar, call]                  |\n[, nyiso, technical, 1/17/01, -, market, eff_dt, comment]                   |\n[, &gt;, -, to:, pm, subject:, please, cc:]                                    |\n[database, initialize, !!!an, occurred, (error, attempting, engine, borland]|\n[, *, ,, please, -, following, to:, subject:]                               |\n[, =20, enron, -, new, please, to:, said]                                   |\n[&gt;, , virus, pnm, western, 1,, miller, virtual]                             |\n[(enw), class=td2, tax), (eel), (ena), (isc), &gt;&lt;td, (corp)]                 |\n[, dog, odor, pet, deborah;, got, hunt, like]                               |\n+----------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">229</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["LDAfunc(11, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.77126280819587\nLog likelihood (higher is better):  -9097851.873196024\n+---------------------------------------------------------------------------------------+\ntopicWords                                                                             |\n+---------------------------------------------------------------------------------------+\n[, to:, subject:, cc:, power, -, state, energy]                                        |\n[(enw), tax), (ena), (eel), (isc), (corp), (corp, (enw]                                |\n[, ok, michelle, 9:13, along, counsel, sent, subject:]                                 |\n[, &gt;, -, to:, subject:, pm, please, cc:]                                               |\n[database, error, occurred, (error, $2a04), !!!an, borland, initialize]                |\n[, ,, please, to:, subject:, &lt;&lt;&lt;&gt;&gt;&gt;, may, -]                                           |\n[, said, enron, =20, new, -, power, million]                                           |\n[&gt;, , virus, 1/17/01, gas, miller, virtual, k]                                         |\n[, shows, 50mw, 25mw, deal, trailing, 6600, desktop]                                   |\n[1,, size=&quot;3&quot;, cellspacing=&quot;0&quot;, , helvetica,, color=&quot;#ffcc33&quot;&gt;&lt;font, &lt;tr, width=&quot;100%&quot;]|\n[class=td2, &gt;&lt;td, &lt;tr, helvetica&quot;&gt;&lt;font, size=1&gt;updated, &gt;&lt;a, bgcolor=#eaeaea, yards]  |\n+---------------------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">230</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["LDAfunc(12, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 42848\nLog perplexity  (lower is better):  6.847568457432971\nLog likelihood (higher is better):  -9200372.077699972\n+--------------------------------------------------------------------------------------------------------------+\ntopicWords                                                                                                    |\n+--------------------------------------------------------------------------------------------------------------+\n[, [image], -, subject:, to:, cc:, desk, please]                                                              |\n[&lt;&lt;&lt;&gt;&gt;&gt;, , 1,, power, contracts, state, report, davis]                                                        |\n[, size=&quot;3&quot;, cellspacing=&quot;0&quot;, helvetica,, color=&quot;#ffcc33&quot;&gt;&lt;font, sans-serif&quot;&gt;&lt;b&gt;&lt;font, bgcolor=&quot;#ffffff&quot;, &lt;tr]|\n[, &gt;, -, to:, subject:, pm, please, enron]                                                                    |\n[database, occurred, $2a04), engine, (error, !!!an, initialize, error]                                        |\n[, ,, ab, *, committee, -, senate, =09=09]                                                                    |\n[, =20, said, new, million, enron, company, --]                                                               |\n[&gt;, (enw), tax), , (ena), (isc), (eel), (corp)]                                                               |\n[, named:, viewing, website., nyiso, technical, western, published]                                           |\n[, terrific, spa., dinner,, afternoon, still, going, whats]                                                   |\n[class=td2, &gt;&lt;td, helvetica&quot;&gt;&lt;font, &gt;&lt;a, &lt;tr, size=1&gt;updated, tax), bgcolor=#f0f0f0]                          |\n[, governor, honest, please, pal, real,, rick, former]                                                        |\n+--------------------------------------------------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">231</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Question 2\nFor stopwords, I attempted topic modeling that did not remove the stopwords, with the pipeline used (without stopword remover) in <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3809935650681405\">Cmd 18</a>, and the results in <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/1835584053677256\">Cmd 19</a>. Results of not removing stopwords, given the initial topic number of 6 resulted in most topic groups being composed of extremely common words(the, to, and, or, ...) , as well as a significant drop of performance seen in log likelihood from the initial setup's -8.9M to -11M, making this a weaker topic grouping. Therefore, it will be better to remove stopwords for this analysis, given their unneeeded prevalence in the topic groups.\n\nFor treatment of non-alpha characters, my approach is to treat all non-alpha characters as if they were whitespace to see if this would improve the topic modeling. The implementation was done through using RegexTokenizer in <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298710\">Cmd 22</a>, using the regular expression [a-zA-Z]+ to capture all alphabet-based strings, while ignoring numbers and punctuation. The weakness of this is that I lose information on emails and compound words with hyphens, but benefits with the ease of use in implementation through a pipeline. The result in <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298711\">Cmd 23</a> shows an improvement in topics, qualitatively with topics that can be identified as scheduling and words associated to some database issue, and quantitatively, with an increase in log likelihood from -8.9M of the original setup to -7.7M."],"metadata":{}},{"cell_type":"code","source":["### LDA (function so k is easily modified)\ndef nostopwords_LDAfunc(kval, data):\n  tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n  vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2) \n  \n  lda = LDA(k=kval, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, vectorizer, lda])\n  pipelineModel = pipeline.fit(data)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n  print(\"Vocab length is\",len(cmv))\n  \n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(data)\n  \n  lp = ldaModel.logPerplexity(enron_lda)\n  print(\"Log perplexity  (lower is better): \",lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  print(\"Log likelihood (higher is better): \",ll)\n  # Describe topics.\n\n  topics = ldaModel.describeTopics(8)\n\n  topics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\n  topics.select(\"topicWords\").show(20,truncate=False)\n  return enron_lda"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["nostopwords_LDAfunc(6, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 43025\nLog perplexity  (lower is better):  6.436347602216701\nLog likelihood (higher is better):  -11787368.09433637\n+------------------------------------------------------------+\ntopicWords                                                  |\n+------------------------------------------------------------+\n[the, of, and, , to, in, a, for]                            |\n[, the, to, class=td2, and, in, for, a]                     |\n[, the, to, and, &gt;, of, a, for]                             |\n[the, , and, to, of, a, =01), &gt;]                            |\n[&lt;&lt;&lt;&gt;&gt;&gt;, 25mw, 9/30, pet, 12/31, anders, culpepper, freeman]|\n[(enw), tax), (eel), (isc), (ena), , the, (corp)]           |\n+------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">233</span><span class=\"ansired\">]: </span>DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, features: vector, topicDistribution: vector]\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["defaulttokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\ndefaulttokens = defaulttokenizer.transform(enron)\ndefaulttokens.select('words').show(1, truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------+\nwords                                |\n+-------------------------------------+\n[test, successful., , way, to, go!!!]|\n+-------------------------------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\nRegextokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", gaps=False, pattern='[a-zA-Z]+')\nregextr = Regextokenizer.transform(enron)\nregextr.select('words').show(1, truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------+\nwords                          |\n+-------------------------------+\n[test, successful, way, to, go]|\n+-------------------------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["def alpha_LDAfunc(kval, data):\n  Regextokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", gaps=False, pattern='[a-zA-Z]+')\n\n  stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  stopWordsRemover.loadDefaultStopWords(\"english\")\n\n  vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2) \n  \n  lda = LDA(k=kval, maxIter=10)\n  pipeline = Pipeline(stages=[Regextokenizer, stopWordsRemover, vectorizer, lda])\n  pipelineModel = pipeline.fit(data)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n  print(\"Vocab length is\",len(cmv))\n  \n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(data)\n  \n  lp = ldaModel.logPerplexity(enron_lda)\n  print(\"Log perplexity  (lower is better): \",lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  print(\"Log likelihood (higher is better): \",ll)\n  # Describe topics.\n\n  topics = ldaModel.describeTopics(8)\n\n  topics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\n  topics.select(\"topicWords\").show(20,truncate=False)\n  \n  return (enron_lda, ldaModel, lp, ll)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["alpha_LDAfunc(6, enron)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 22775\nLog perplexity  (lower is better):  8.34189058938049\nLog likelihood (higher is better):  -7728351.149790565\n+-------------------------------------------------------------+\ntopicWords                                                   |\n+-------------------------------------------------------------+\n[error, database, td, font, br, engine, attempting, occurred]|\n[nyiso, power, com, ect, iso, enron, hou, said]              |\n[enron, ect, com, hou, subject, pm, please, cc]              |\n[munder, com, please, subject, roast, enron, http, message]  |\n[com, company, said, e, http, enron, www, new]               |\n[ect, final, type, trans, schedule, date, hou, hour]         |\n+-------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">248</span><span class=\"ansired\">]: </span>\n(DataFrame[uuid: string, from: string, to: array&lt;string&gt;, cc: array&lt;string&gt;, bcc: array&lt;string&gt;, dateUtcEpoch: bigint, subject: string, mailFields: map&lt;string,string&gt;, body: string, attachments: array&lt;struct&lt;fileName:string,size:int,mimeType:string,data:binary&gt;&gt;, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector],\n LDA_44d1bd23e3dace5e55c7,\n 8.34189058938049,\n -7728351.149790565)\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["### Question 3\nFor this question, the quoted messages were cleaned through using a map function and the split function, targeting the first part of the header used in quoted messages ('-----Original' as indicated in some messages and in Piazza). The 1st item in the list was then indexed and used as the body of the message. \n\nThe result of removing the quoted message is a drastic improvement in log likelihood from the initial setup's -8.9M to -5.9M. Topics in <a href='https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298712'>Cmd 26</a> seem to be more focused as well, with a very clear topic on power and electricity being defined and a topic focusing on time/scheduling."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nb = spark.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").select('body').rdd\nb = b.map(lambda x: x.body.split('-----Original')[0])\ncleaned = Row('body')\nclean_enron = b.map(cleaned).toDF()\nsampled_clean = clean_enron.sample(False,0.01)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["alpha_LDAfunc(6, sampled_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 19534\nLog perplexity  (lower is better):  8.339777617079315\nLog likelihood (higher is better):  -5879108.816861954\n+----------------------------------------------------------------------+\ntopicWords                                                            |\n+----------------------------------------------------------------------+\n[enron, com, ect, please, pm, e, subject, http]                       |\n[mckinsey, com, e, enron, fool, northamerica, mail, power]            |\n[final, schedules, schedule, date, type, id, trans, hour]             |\n[ect, hou, enron, ees, communications, corp, edu, cc]                 |\n[power, state, said, california, energy, electricity, utilities, http]|\n[image, ect, newpower, enron, hou, please, com, nc]                   |\n+----------------------------------------------------------------------+\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">253</span><span class=\"ansired\">]: </span>\n(DataFrame[body: string, words: array&lt;string&gt;, filtered: array&lt;string&gt;, features: vector, topicDistribution: vector],\n LDA_4342a098cc324bd2e5c1,\n 8.339777617079315,\n -5879108.816861954)\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### Question 4\nBased on 5 1% samples of the whole Enron email corpus, the topics from multiple runs are still unstable, with no topic being found in all 5 samples, which may indicate a need to revisit cleaning the data, or that 1% may be too low to generalize the email corpus. Consistency/stability in this case is defined as a topic being easily identified within all 5 samples. In terms of quantitative analyses, the topic modeling is decently stable, with log likelihood ranging from -6.3M to -5.8M\n\nThe following qualitative judgements were made on the samples below, and the topics that can be clearly developed.\n\nSample 1 topics (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298719\">Cmd 29</a>): Planned outages (topic 1), A game/football? (topic 4)\n\nSample 2 topics (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298720\">Cmd 30</a>): Football (topic 4), database issues (topic 5), something about gas/temperature (topic 6)\n\nSample 3 topics (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298721\">Cmd 31</a>): Time words (topic 2), Fantasy sports (topic 4), California (topic 6)\n\nSample 4 topics (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298722\">Cmd 32</a>): Football (topic 4), Energy words (topic 5), Customer sales (topic 8)\n\nSample 5 topics (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298723\">Cmd 33</a>): Employeees (topic 2), travel (topic 3), time (topic 5), HTML tags (topic 7)\n\nOf the topics listed here, sports/football seemed to be the most stable topic, appearing in 4 of the sampled LDAs, probably owing to what seems to be a Fantasy Football league with regular emails. Second most mentioned topic would be time words, focused on schedules and dates, as seen in Sample 2 and 5. This may indicate that Enron's emails may have some regularity, in a regular fantasy league email, and a need to be updated on meetings through email."],"metadata":{}},{"cell_type":"code","source":["sample1 = clean_enron.sample(False,0.01)\nsample2 = clean_enron.sample(False,0.01)\nsample3 = clean_enron.sample(False,0.01)\nsample4 = clean_enron.sample(False,0.01)\nsample5 = clean_enron.sample(False,0.01)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["LDA1 = alpha_LDAfunc(9, sample1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 19314\nLog perplexity  (lower is better):  8.460461754282074\nLog likelihood (higher is better):  -5942732.834475212\n+----------------------------------------------------------------+\ntopicWords                                                      |\n+----------------------------------------------------------------+\n[enron, pm, et, thru, scheduled, outages, sat, ct]              |\n[image, se, sp, wscc, gif, ne, hp, ctr]                         |\n[feedback, positions, pep, please, ect, know, intramonth, basis]|\n[updated, game, week, fantasy, wr, sunday, play, rb]            |\n[ect, hou, cgas, enron, week, wordsmith, deal, subject]         |\n[com, http, www, please, aa, worth, week, may]                  |\n[com, br, unk, enron, p, font, nuke, please]                    |\n[enron, ect, com, hou, please, cc, http, subject]               |\n[enron, ect, hou, com, ees, market, jpg, week]                  |\n+----------------------------------------------------------------+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["LDA2 = alpha_LDAfunc(9, sample2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 19254\nLog perplexity  (lower is better):  8.522455862081184\nLog likelihood (higher is better):  -5881602.114192836\n+-----------------------------------------------------------------+\ntopicWords                                                       |\n+-----------------------------------------------------------------+\n[enron, ect, com, hou, please, e, pm, subject]                   |\n[expedia, ect, rfrr, gif, cb, need, enron, cq]                   |\n[ak, wa, slice, ca, day, dwp, bpa, rule]                         |\n[fantasy, td, game, yards, passing, team, league, nfl]           |\n[database, data, dbcaps, nyiso, unknown, alias, error, operation]|\n[nov, m, oct, feedback, j, hayslett, tree, james]                |\n[mmbtu, d, ect, doc, deliveries, hou, cold, capacity]            |\n[ca, com, ak, enron, wa, edu, berkeley, fares]                   |\n[ect, enron, final, schedule, schedules, date, hour, hou]        |\n+-----------------------------------------------------------------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["LDA3 = alpha_LDAfunc(9, sample3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 20304\nLog perplexity  (lower is better):  8.420927779328013\nLog likelihood (higher is better):  -6173779.423931431\n+-------------------------------------------------------------+\ntopicWords                                                   |\n+-------------------------------------------------------------+\n[com, enron, energy, power, d, new, e, said]                 |\n[final, type, data, date, schedule, trans, dbcaps, schedules]|\n[cn, com, e, mail, dna, energy, market, ect]                 |\n[http, www, com, gif, html, nytimes, sportsline, fantasy]    |\n[com, enron, operation, go, yahoo, going, know, doc]         |\n[sheraton, com, time, please, enron, program, p, pm]         |\n[epmi, term, com, total, p, southwest, times, palo]          |\n[enron, ect, hou, com, subject, cc, corp, ees]               |\n[ect, enron, kate, please, prebon, hou, subject, deal]       |\n+-------------------------------------------------------------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["LDA4 = alpha_LDAfunc(9, sample4)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 19551\nLog perplexity  (lower is better):  8.455983765054949\nLog likelihood (higher is better):  -5983618.538571332\n+---------------------------------------------------------+\ntopicWords                                               |\n+---------------------------------------------------------+\n[enron, com, font, td, http, br, pm, tr]                 |\n[ect, enron, hou, corp, na, ees, enronxgate, subject]    |\n[enron, com, said, na, ferc, power, dow, dasovich]       |\n[applewhite, kc, freshmen, longhorn, yds, yards, e, true]|\n[error, image, power, database, enron, gas, data, energy]|\n[enron, com, ect, d, e, please, http, subject]           |\n[com, market, enron, price, gas, group, may, please]     |\n[gas, image, team, managed, manage, ooc, ea, continue]   |\n[com, customer, id, request, ref, aol, seller, fc]       |\n+---------------------------------------------------------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["LDA5 = alpha_LDAfunc(9, sample5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 20753\nLog perplexity  (lower is better):  8.416464601966066\nLog likelihood (higher is better):  -6361641.375780708\n+-------------------------------------------------------------+\ntopicWords                                                   |\n+-------------------------------------------------------------+\n[enron, com, http, e, said, company, please, energy]         |\n[prc, get, associate, analyst, associates, months, thanks, m]|\n[specials, ca, hilton, day, travel, airport, hotel, rates]   |\n[d, br, font, e, week, pm, enron, may]                       |\n[enron, error, please, know, type, final, date, schedule]    |\n[ect, hou, enron, ees, cc, image, subject, pm]               |\n[com, day, director, enron, tana, please, taht, susan]       |\n[font, td, tr, width, b, href, size, br]                     |\n[enron, com, et, ect, na, pm, corp, subject]                 |\n+-------------------------------------------------------------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["### Above and Beyond (Document Identification)\nData used: 5% of Enron dataset\n\nProcess used: Put topic distribution back to Pandas, and used np.argmax + 1 to do document labeling (argmax to get index of highest topic distribution value) (<a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298726\")>Cmd 37</a> )\n\nRaw results are on <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298726\")>Cmd 37</a>, and graph representing results are on <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298729\")>Cmd 38</a>\n\nBased on the topics in <a href=\"https://community.cloud.databricks.com/?o=4609019411349203#notebook/3809935650681380/command/3937380071298725\")>Cmd 36</a>:\n* Topic 5 (18,804 documents) has the most documents classified under it, and seems to be a group of forwarded emails, given the prevalence of cc, subject, and please as potentially common parts of the header and beginning of an email. This may point to a large portion of the email corpus being forwarded emails and replies to that, which may necessitate additional data cleaning for better topic grouping.\n* Topic 1 (4,197 documents) is the 2nd highest, where the topic seems to be descriptions of Enron's general operations, indicating their participation in power and energy, and their presence in California.)\n* While Topic 6 (1,943 documents) is a large collection of documents, the topic itself does not seem to be well-formed, given the random nature of the included words\n* Topic 9 (488 documents) is a well-defined topic, covering schedules and times again, similarly to Question 4"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\ntester = LDA1[0].select(LDA1[0]['topicDistribution']).toPandas()\ntester['doc'] = tester['topicDistribution'].apply(np.argmax) + 1\ntester"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">270</span><span class=\"ansired\">]: </span>\n                                      topicDistribution  doc\n0     [0.00180860124621, 0.00175657850042, 0.0017445...    8\n1     [0.00108034564097, 0.0010493079505, 0.00104204...    8\n2     [0.0023139972352, 0.00224762442579, 0.00223201...    8\n3     [0.0341195811189, 0.0331415191601, 0.032911928...    8\n4     [0.000649402968488, 0.000630720445304, 0.00062...    8\n5     [0.0041533387533, 0.00403395302596, 0.00400618...    8\n6     [0.0100191735245, 0.00973168931426, 0.00966425...    8\n7     [0.00769141364793, 0.00747192574597, 0.0074190...    8\n8     [0.00284443263173, 0.00276273357573, 0.0027435...    8\n9     [8.60664615764e-05, 8.35899433716e-05, 8.30111...    8\n10    [0.00587279507394, 0.00570373170477, 0.0056640...    8\n11    [0.00433421264977, 0.00420968666598, 0.0041804...    8\n12    [0.000520158943152, 0.000505190405203, 0.00050...    8\n13    [0.000153272794787, 0.000148856324041, 0.00014...    8\n14    [0.0021628492283, 0.00210070035841, 0.00208615...    8\n15    [0.00048945414216, 0.000475365673776, 0.000472...    8\n16    [0.000856631864965, 0.000832061645874, 0.00082...    8\n17    [0.00211666726262, 0.00205597080846, 0.0020417...    8\n18    [0.0024271583591, 0.00235744883565, 0.00234108...    8\n19    [0.000445449762262, 0.000432662981881, 0.00042...    8\n20    [0.000254022223793, 0.593311577126, 0.00024500...    2\n21    [0.0032125819213, 0.00312017038222, 0.00309859...    8\n22    [0.000928779227525, 0.000902085568392, 0.00089...    8\n23    [0.00355762713032, 0.00345545831618, 0.0034315...    8\n24    [0.00383244400709, 0.00372235758079, 0.0036966...    8\n25    [0.00474868105675, 0.00461228724011, 0.0045803...    8\n26    [0.0100198555678, 0.0097327415393, 0.009665735...    8\n27    [0.0029281515208, 0.0028441223164, 0.002824427...    8\n28    [0.00140030004749, 0.00136011462421, 0.0013507...    8\n29    [0.0202235887032, 0.0196425920152, 0.019506729...    9\n...                                                 ...  ...\n5225  [0.00144100300061, 0.00139962930683, 0.0013899...    8\n5226  [0.0168020485263, 0.0163207090413, 0.016207680...    8\n5227  [0.00124274555414, 0.340719283077, 0.001198719...    8\n5228  [0.00236951270625, 0.00230152086411, 0.4372379...    8\n5229  [0.00665945601301, 0.00646838460145, 0.0064234...    8\n5230  [0.00146224911924, 0.00142027092033, 0.0014104...    8\n5231  [0.00713871143201, 0.00693375224178, 0.0068863...    8\n5232  [0.00127455998555, 0.00123791942814, 0.0012293...    8\n5233      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]    1\n5234  [0.00168593673108, 0.00163730743957, 0.0016260...    8\n5235  [0.00624137490158, 0.00606196422702, 0.0060205...    8\n5236  [0.00124259800944, 0.00120690794855, 0.0011985...    8\n5237  [0.00666001500774, 0.00646876497156, 0.0064240...    8\n5238  [0.00138081742406, 0.00134113603145, 0.0013318...    8\n5239  [0.0143702045684, 0.0139569016218, 0.013860454...    8\n5240  [0.310862352532, 0.0139586603341, 0.0138625016...    7\n5241  [0.0091021513506, 0.00884065233498, 0.13320692...    8\n5242  [0.00269032024135, 0.00261314520144, 0.0025951...    8\n5243  [0.00415283915462, 0.00403347298353, 0.0040057...    8\n5244  [0.000301025070249, 0.0369797287806, 0.0002903...    8\n5245  [0.00119759539833, 0.00116322452785, 0.0011551...    8\n5246  [0.00221099348804, 0.00214751876215, 0.0021326...    8\n5247  [0.0341215245385, 0.0331435016952, 0.032913420...    8\n5248  [0.000613362659136, 0.000595693820979, 0.00059...    8\n5249  [0.000903575734592, 0.000877556731007, 0.00087...    8\n5250  [0.00311182553043, 0.0030224950099, 0.00300141...    8\n5251  [0.00174491761294, 0.00169483317299, 0.0016831...    8\n5252  [0.601808328652, 0.000117337870992, 0.00011652...    1\n5253  [0.00769190826907, 0.00747046754438, 0.0074187...    8\n5254  [0.000531326836323, 0.000516022137803, 0.00051...    8\n\n[5255 rows x 2 columns]\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["enron_5 = alpha_LDAfunc(9, clean_enron.sample(False,0.05))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Vocab length is 47692\nLog perplexity  (lower is better):  8.287987631689054\nLog likelihood (higher is better):  -30311352.699408546\n+-------------------------------------------------------------------+\ntopicWords                                                         |\n+-------------------------------------------------------------------+\n[enron, power, energy, new, said, company, com, california]        |\n[enron, com, ect, pec, hou, e, ees, communications]                |\n[travelocity, folder, synchronizing, http, com, www, font, edu]    |\n[com, e, mail, ect, energy, enron, market, area]                   |\n[com, enron, please, ect, subject, e, pm, cc]                      |\n[ect, enron, hou, corp, na, ees, cc, pm]                           |\n[enron, gas, mmbtu, schedules, california, file, company, portland]|\n[td, font, b, br, http, size, class, com]                          |\n[final, type, trans, schedule, date, id, schedules, hour]          |\n+-------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["topic_dist = enron_5[0].select(enron_5[0]['topicDistribution']).toPandas()\ntopic_dist['doc'] = topic_dist['topicDistribution'].apply(np.argmax) + 1\ntopic_dist['doc'].value_counts()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">302</span><span class=\"ansired\">]: </span>\n5    18804\n1     4197\n6     1943\n9      488\n8      185\n3       35\n2       32\n7       18\n4       14\nName: doc, dtype: int64\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.figure()\nfinalcount = topic_dist['doc'].value_counts().sort_index().plot(kind='bar')\ndisplay()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["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"]}}],"execution_count":38}],"metadata":{"name":"HW8_rapku","notebookId":3809935650681380},"nbformat":4,"nbformat_minor":0}
